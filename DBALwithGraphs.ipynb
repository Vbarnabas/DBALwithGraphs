{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "def load_tu_data(dataset_name='PROTEINS', use_node_attr=True):\n",
    "    dataset = TUDataset(root='/tmp/' + dataset_name, name=dataset_name, use_node_attr=use_node_attr)\n",
    "\n",
    "    num_classes = dataset.num_classes\n",
    "    num_node_features = dataset.num_features\n",
    "\n",
    "    num_total = len(dataset)\n",
    "    num_train = int(num_total * 0.3)\n",
    "    num_val = int(num_total * 0.01)\n",
    "    num_test = int(num_total * 0.1)\n",
    "    num_pool = num_total - num_train - num_val - num_test\n",
    "\n",
    "    indices = torch.randperm(num_total).tolist()\n",
    "    train_indices = indices[:num_train]\n",
    "    val_indices = indices[num_train:num_train + num_val]\n",
    "    test_indices = indices[num_train + num_val:num_train + num_val + num_test]\n",
    "    pool_indices = indices[num_train + num_val + num_test:]\n",
    "\n",
    "    return dataset, train_indices, val_indices, test_indices, pool_indices, num_classes, num_node_features\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T17:36:40.131855300Z",
     "start_time": "2024-05-27T17:36:40.047684200Z"
    }
   },
   "id": "a20cc50dc2ad9948"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Indice alapj√°n Data Loader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73c7a75eec7088aa"
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "def create_data_loader(dataset, indices, batch_size=10, shuffle=True):\n",
    "    subset = torch.utils.data.Subset(dataset, indices)\n",
    "    return DataLoader(subset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "def update_indices(train_indices, pool_indices, selected_pool_indices):\n",
    "    \"\"\"\n",
    "    Update training and pool indices after selecting some indices from the pool.\n",
    "    \n",
    "    Args:\n",
    "        train_indices (list): Current list of training indices.\n",
    "        pool_indices (list): Current list of pool indices.\n",
    "        selected_pool_indices (list): Indices selected from the pool to be moved to training.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Updated lists of train and pool indices.\n",
    "    \"\"\"\n",
    "    # Ensure all are lists\n",
    "    train_indices = list(train_indices)\n",
    "    selected_pool_indices = list(selected_pool_indices)\n",
    "\n",
    "    # Add selected indices to train indices\n",
    "    new_train_indices = train_indices + selected_pool_indices\n",
    "\n",
    "    # Remove the selected indices from pool indices\n",
    "    new_pool_indices = [idx for idx in pool_indices if idx not in selected_pool_indices]\n",
    "\n",
    "    return new_train_indices, new_pool_indices\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T17:36:40.317870100Z",
     "start_time": "2024-05-27T17:36:40.076223400Z"
    }
   },
   "id": "f705bf422b0873f5"
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes, hidden_channels=64):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.out = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        # Global mean pooling\n",
    "        x = global_mean_pool(x, batch)  # Aggregate node features to graph features\n",
    "        \n",
    "        x = self.out(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T17:36:40.355393700Z",
     "start_time": "2024-05-27T17:36:40.086422800Z"
    }
   },
   "id": "83e416079edf0253"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Uniform Acquisition function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a646e28150e15ae7"
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def uniform(model, dataset, pool_indices, n_query, T=100, training=True):\n",
    "    \"\"\"\n",
    "    Uniformly random selection of data points from the unlabeled pool.\n",
    "    \n",
    "    Args:\n",
    "    pool_indices (list): List of indices available in the pool.\n",
    "    n_query (int): Number of queries to make.\n",
    "    \n",
    "    Returns:\n",
    "    list: Indices of the selected data points.\n",
    "    \"\"\"\n",
    "    # Directly use the pool_indices to select data points\n",
    "    selected_indices = np.random.choice(pool_indices, size=n_query, replace=False)\n",
    "    \n",
    "    return selected_indices.tolist()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T17:36:40.356394900Z",
     "start_time": "2024-05-27T17:36:40.096627300Z"
    }
   },
   "id": "746a13411f73ad02"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Active learning loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c06d6f29d8eca10f"
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "def active_learning_loop(model, dataset, train_indices, pool_indices, val_indices, test_indices, query_strategy, n_query=10, epochs=100):\n",
    "    train_loader = create_data_loader(dataset, train_indices)\n",
    "    val_loader = create_data_loader(dataset, val_indices, shuffle=False)\n",
    "    test_loader = create_data_loader(dataset, test_indices, shuffle=False)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = criterion(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "             \n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            val_acc = evaluate_model(model, val_loader)\n",
    "            print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Val Accuracy: {val_acc:.4f}')\n",
    "            selected_indices = query_strategy(model, dataset, pool_indices, n_query, T=100, training=True)\n",
    "            print(f'Selected indices for training: {selected_indices}')\n",
    "            train_indices, pool_indices = update_indices(train_indices, pool_indices, selected_indices)\n",
    "            print(f'Updated train indices count: {len(train_indices)}, Pool indices count: {len(pool_indices)}')\n",
    "            train_loader = create_data_loader(dataset, train_indices)  # Recreate the loader with updated indices\n",
    "\n",
    "    test_acc = evaluate_model(model, test_loader)\n",
    "    print(f'Final Test Accuracy: {test_acc:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T17:36:40.399556900Z",
     "start_time": "2024-05-27T17:36:40.111730200Z"
    }
   },
   "id": "f649ad162ffaad6b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bba1350f7187d6e2"
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in loader:\n",
    "        batch.to(device) # Ensure labels are also moved\n",
    "        out = model(batch)\n",
    "        pred = out.max(dim=1)[1]\n",
    "        correct += pred.eq(batch.y).sum().item()\n",
    "        total += batch.num_graphs\n",
    "    return correct / total\n",
    "\n",
    "def update_loaders(train_loader, pool_loader, new_data_indices, pool_indices, train_indices):\n",
    "    # Add new data indices to train indices\n",
    "    for idx in new_data_indices:\n",
    "        train_indices.append(pool_indices[idx])\n",
    "\n",
    "    # Remove the selected indices from pool indices\n",
    "    new_pool_indices = [idx for i, idx in enumerate(pool_indices) if i not in new_data_indices]\n",
    "\n",
    "    # Update datasets and loaders\n",
    "    train_loader.dataset.indices = train_indices\n",
    "    pool_loader.dataset.indices = new_pool_indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T17:36:40.421889300Z",
     "start_time": "2024-05-27T17:36:40.126013900Z"
    }
   },
   "id": "22c29ad0d3fe1375"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 23.3080, Val Accuracy: 0.4545\n",
      "Selected indices for training: [676, 85, 77, 867, 793, 426, 812, 239, 246, 448]\n",
      "Updated train indices count: 343, Pool indices count: 648\n",
      "Epoch: 11, Train Loss: 22.3764, Val Accuracy: 0.4545\n",
      "Selected indices for training: [609, 650, 400, 810, 671, 631, 302, 658, 160, 177]\n",
      "Updated train indices count: 353, Pool indices count: 638\n",
      "Epoch: 21, Train Loss: 22.8176, Val Accuracy: 0.4545\n",
      "Selected indices for training: [173, 966, 513, 415, 784, 943, 138, 147, 755, 68]\n",
      "Updated train indices count: 363, Pool indices count: 628\n",
      "Epoch: 31, Train Loss: 23.0951, Val Accuracy: 0.4545\n",
      "Selected indices for training: [1088, 441, 728, 1108, 1000, 298, 29, 14, 307, 801]\n",
      "Updated train indices count: 373, Pool indices count: 618\n"
     ]
    }
   ],
   "source": [
    "dataset, train_indices, val_indices, test_indices, pool_indices, num_classes, num_node_features = load_tu_data()\n",
    "model = GNN(num_node_features, num_classes).to(device)\n",
    "active_learning_loop(model, dataset, train_indices, pool_indices, val_indices, test_indices, uniform, epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-27T17:36:40.136861800Z"
    }
   },
   "id": "a388e9b143003e21"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Max Entropy Acquisition function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a154e65406c8c63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predictions_from_pool(model, dataset, pool_indices, T=100, training=True):\n",
    "    \"\"\"\n",
    "    Run MC dropout prediction on model using graphs from the pool and return the output.\n",
    "    \"\"\"\n",
    "    # Randomly select indices from the pool\n",
    "    random_subset = np.random.choice(pool_indices, size=min(2000, len(pool_indices)), replace=False)\n",
    "    \n",
    "    # Fetch the actual graph data from the dataset\n",
    "    subset_loader = DataLoader(dataset[random_subset.tolist()], batch_size=len(random_subset), shuffle=False)\n",
    "    batch = next(iter(subset_loader))  # Load the batch\n",
    "    \n",
    "    # Perform prediction\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(T):\n",
    "            batch.to(device)\n",
    "            model.train(training)  # Enable/disable dropout\n",
    "            output = torch.softmax(model(batch), dim=-1)\n",
    "            outputs.append(output.cpu().numpy())\n",
    "    outputs = np.stack(outputs)\n",
    "    print(outputs.shape)\n",
    "    return outputs, random_subset\n",
    "\n",
    "def shannon_entropy_function(model, dataset, pool_indices, T=100, E_H=False, training=True):\n",
    "    \"\"\"\n",
    "    Compute the Shannon entropy and optionally E_H if needed for BALD.\n",
    "    \"\"\"\n",
    "    outputs, random_subset = predictions_from_pool(model, dataset, pool_indices, T, training)\n",
    "    pc = outputs.mean(axis=0)\n",
    "    H = (-pc * np.log(pc + 1e-10)).sum(axis=-1)  # Prevent log(0)\n",
    "\n",
    "    if E_H:\n",
    "        E = -np.mean(np.sum(outputs * np.log(outputs + 1e-10), axis=-1), axis=0)\n",
    "        return H, E, random_subset\n",
    "    return H, random_subset\n",
    "\n",
    "def max_entropy(model, dataset, pool_indices, n_query=10, T=100, training=True):\n",
    "    \"\"\"\n",
    "    Choose pool points that maximize the predictive entropy.\n",
    "    \"\"\"\n",
    "    acquisition, random_subset = shannon_entropy_function(model, dataset, pool_indices, T, training=training)\n",
    "    idx = (-acquisition).argsort()[:n_query]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "bec59d22cfd25cbd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training with Max Entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "644a17254074a03e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset, train_indices, val_indices, test_indices, pool_indices, num_classes, num_node_features = load_tu_data()\n",
    "model = GNN(num_node_features, num_classes).to(device)\n",
    "active_learning_loop(model, dataset, train_indices, pool_indices, val_indices, test_indices, max_entropy, epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ec61af1d1265047f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b68ee0ff709dad61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
