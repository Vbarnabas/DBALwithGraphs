{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "def load_tu_data(dataset_name='PROTEINS', use_node_attr=True):\n",
    "    dataset = TUDataset(root='/tmp/' + dataset_name, name=dataset_name, use_node_attr=use_node_attr)\n",
    "\n",
    "    num_classes = dataset.num_classes\n",
    "    num_node_features = dataset.num_features\n",
    "\n",
    "    num_total = len(dataset)\n",
    "    num_train = int(num_total * 0.6)\n",
    "    num_val = int(num_total * 0.1)\n",
    "    num_test = int(num_total * 0.1)\n",
    "    num_pool = num_total - num_train - num_val - num_test\n",
    "\n",
    "    indices = torch.randperm(num_total).tolist()\n",
    "    train_indices = indices[:num_train]\n",
    "    val_indices = indices[num_train:num_train + num_val]\n",
    "    test_indices = indices[num_train + num_val:num_train + num_val + num_test]\n",
    "    pool_indices = indices[num_train + num_val + num_test:]\n",
    "\n",
    "    return dataset, train_indices, val_indices, test_indices, pool_indices, num_classes, num_node_features\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T17:29:18.897661500Z",
     "start_time": "2024-05-27T17:29:18.735182400Z"
    }
   },
   "id": "a20cc50dc2ad9948"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Indice alapj√°n Data Loader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73c7a75eec7088aa"
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "def create_data_loader(dataset, indices, batch_size=10, shuffle=True):\n",
    "    subset = torch.utils.data.Subset(dataset, indices)\n",
    "    return DataLoader(subset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "def update_indices(train_indices, pool_indices, selected_pool_indices):\n",
    "    \"\"\"\n",
    "    Update training and pool indices after selecting some indices from the pool.\n",
    "    \n",
    "    Args:\n",
    "        train_indices (list): Current list of training indices.\n",
    "        pool_indices (list): Current list of pool indices.\n",
    "        selected_pool_indices (list): Indices selected from the pool to be moved to training.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Updated lists of train and pool indices.\n",
    "    \"\"\"\n",
    "    # Ensure all are lists\n",
    "    train_indices = list(train_indices)\n",
    "    selected_pool_indices = list(selected_pool_indices)\n",
    "\n",
    "    # Add selected indices to train indices\n",
    "    new_train_indices = train_indices + selected_pool_indices\n",
    "\n",
    "    # Remove the selected indices from pool indices\n",
    "    new_pool_indices = [idx for idx in pool_indices if idx not in selected_pool_indices]\n",
    "\n",
    "    return new_train_indices, new_pool_indices\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T17:29:18.930222800Z",
     "start_time": "2024-05-27T17:29:18.752816400Z"
    }
   },
   "id": "f705bf422b0873f5"
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes, hidden_channels=64):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.out = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        # Global mean pooling\n",
    "        x = global_mean_pool(x, batch)  # Aggregate node features to graph features\n",
    "        \n",
    "        x = self.out(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T17:29:19.090411500Z",
     "start_time": "2024-05-27T17:29:18.763149700Z"
    }
   },
   "id": "83e416079edf0253"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Uniform Acquisition function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a646e28150e15ae7"
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def uniform(model, dataset, pool_indices, n_query, T=100, training=True):\n",
    "    \"\"\"\n",
    "    Uniformly random selection of data points from the unlabeled pool.\n",
    "    \n",
    "    Args:\n",
    "    pool_indices (list): List of indices available in the pool.\n",
    "    n_query (int): Number of queries to make.\n",
    "    \n",
    "    Returns:\n",
    "    list: Indices of the selected data points.\n",
    "    \"\"\"\n",
    "    # Directly use the pool_indices to select data points\n",
    "    selected_indices = np.random.choice(pool_indices, size=n_query, replace=False)\n",
    "    \n",
    "    return selected_indices.tolist()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T17:29:19.129144Z",
     "start_time": "2024-05-27T17:29:18.771864300Z"
    }
   },
   "id": "746a13411f73ad02"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Active learning loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c06d6f29d8eca10f"
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "def active_learning_loop(model, dataset, train_indices, pool_indices, val_indices, test_indices, query_strategy, n_query=10, epochs=100):\n",
    "    train_loader = create_data_loader(dataset, train_indices)\n",
    "    val_loader = create_data_loader(dataset, val_indices, shuffle=False)\n",
    "    test_loader = create_data_loader(dataset, test_indices, shuffle=False)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = criterion(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "             \n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            val_acc = evaluate_model(model, val_loader)\n",
    "            print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Val Accuracy: {val_acc:.4f}')\n",
    "            selected_indices = query_strategy(model, dataset, pool_indices, n_query, T=100, training=True)\n",
    "            print(f'Selected indices for training: {selected_indices}')\n",
    "            train_indices, pool_indices = update_indices(train_indices, pool_indices, selected_indices)\n",
    "            print(f'Updated train indices count: {len(train_indices)}, Pool indices count: {len(pool_indices)}')\n",
    "            train_loader = create_data_loader(dataset, train_indices)  # Recreate the loader with updated indices\n",
    "\n",
    "    test_acc = evaluate_model(model, test_loader)\n",
    "    print(f'Final Test Accuracy: {test_acc:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T17:29:19.154337700Z",
     "start_time": "2024-05-27T17:29:18.785000600Z"
    }
   },
   "id": "f649ad162ffaad6b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bba1350f7187d6e2"
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in loader:\n",
    "        batch.to(device) # Ensure labels are also moved\n",
    "        out = model(batch)\n",
    "        pred = out.max(dim=1)[1]\n",
    "        correct += pred.eq(batch.y).sum().item()\n",
    "        total += batch.num_graphs\n",
    "    return correct / total\n",
    "\n",
    "def update_loaders(train_loader, pool_loader, new_data_indices, pool_indices, train_indices):\n",
    "    # Add new data indices to train indices\n",
    "    for idx in new_data_indices:\n",
    "        train_indices.append(pool_indices[idx])\n",
    "\n",
    "    # Remove the selected indices from pool indices\n",
    "    new_pool_indices = [idx for i, idx in enumerate(pool_indices) if i not in new_data_indices]\n",
    "\n",
    "    # Update datasets and loaders\n",
    "    train_loader.dataset.indices = train_indices\n",
    "    pool_loader.dataset.indices = new_pool_indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T17:29:19.154337700Z",
     "start_time": "2024-05-27T17:29:18.797196100Z"
    }
   },
   "id": "22c29ad0d3fe1375"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 49.4214, Val Accuracy: 0.6847\n",
      "Selected indices for training: [205, 465, 1092, 472, 983, 587, 138, 544, 583, 326]\n",
      "Updated train indices count: 677, Pool indices count: 214\n",
      "Epoch: 11, Train Loss: 45.6071, Val Accuracy: 0.6757\n",
      "Selected indices for training: [419, 834, 686, 844, 130, 407, 1056, 268, 594, 369]\n",
      "Updated train indices count: 687, Pool indices count: 204\n",
      "Epoch: 21, Train Loss: 44.9365, Val Accuracy: 0.6847\n",
      "Selected indices for training: [107, 581, 744, 585, 169, 827, 247, 967, 227, 322]\n",
      "Updated train indices count: 697, Pool indices count: 194\n",
      "Epoch: 31, Train Loss: 44.6771, Val Accuracy: 0.7477\n",
      "Selected indices for training: [1033, 710, 578, 696, 84, 33, 60, 224, 79, 1059]\n",
      "Updated train indices count: 707, Pool indices count: 184\n",
      "Epoch: 41, Train Loss: 45.0485, Val Accuracy: 0.7027\n",
      "Selected indices for training: [968, 129, 476, 1057, 276, 541, 292, 662, 327, 939]\n",
      "Updated train indices count: 717, Pool indices count: 174\n",
      "Epoch: 51, Train Loss: 44.3233, Val Accuracy: 0.6757\n",
      "Selected indices for training: [323, 391, 189, 928, 786, 271, 1024, 1110, 445, 179]\n",
      "Updated train indices count: 727, Pool indices count: 164\n",
      "Epoch: 61, Train Loss: 45.6182, Val Accuracy: 0.7297\n",
      "Selected indices for training: [170, 613, 910, 93, 152, 781, 522, 1054, 171, 124]\n",
      "Updated train indices count: 737, Pool indices count: 154\n",
      "Epoch: 71, Train Loss: 45.7834, Val Accuracy: 0.7748\n",
      "Selected indices for training: [862, 506, 560, 1068, 1107, 579, 287, 144, 231, 489]\n",
      "Updated train indices count: 747, Pool indices count: 144\n",
      "Epoch: 81, Train Loss: 45.7826, Val Accuracy: 0.7658\n",
      "Selected indices for training: [381, 923, 337, 382, 600, 505, 627, 204, 886, 398]\n",
      "Updated train indices count: 757, Pool indices count: 134\n",
      "Epoch: 91, Train Loss: 47.4554, Val Accuracy: 0.6937\n",
      "Selected indices for training: [720, 70, 921, 1104, 507, 408, 435, 182, 491, 453]\n",
      "Updated train indices count: 767, Pool indices count: 124\n",
      "Epoch: 101, Train Loss: 46.1882, Val Accuracy: 0.7207\n",
      "Selected indices for training: [749, 549, 765, 19, 75, 50, 374, 244, 832, 331]\n",
      "Updated train indices count: 777, Pool indices count: 114\n",
      "Epoch: 111, Train Loss: 47.3191, Val Accuracy: 0.7117\n",
      "Selected indices for training: [457, 173, 30, 94, 289, 791, 216, 754, 1002, 1103]\n",
      "Updated train indices count: 787, Pool indices count: 104\n",
      "Epoch: 121, Train Loss: 48.5879, Val Accuracy: 0.7297\n",
      "Selected indices for training: [576, 610, 109, 551, 964, 21, 396, 263, 758, 931]\n",
      "Updated train indices count: 797, Pool indices count: 94\n"
     ]
    }
   ],
   "source": [
    "dataset, train_indices, val_indices, test_indices, pool_indices, num_classes, num_node_features = load_tu_data()\n",
    "model = GNN(num_node_features, num_classes).to(device)\n",
    "active_learning_loop(model, dataset, train_indices, pool_indices, val_indices, test_indices, uniform, epochs=200)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-27T17:29:18.808040700Z"
    }
   },
   "id": "a388e9b143003e21"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Max Entropy Acquisition function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a154e65406c8c63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predictions_from_pool(model, dataset, pool_indices, T=100, training=True):\n",
    "    \"\"\"\n",
    "    Run MC dropout prediction on model using graphs from the pool and return the output.\n",
    "    \"\"\"\n",
    "    # Randomly select indices from the pool\n",
    "    random_subset = np.random.choice(pool_indices, size=min(2000, len(pool_indices)), replace=False)\n",
    "    \n",
    "    # Fetch the actual graph data from the dataset\n",
    "    subset_loader = DataLoader(dataset[random_subset.tolist()], batch_size=len(random_subset), shuffle=False)\n",
    "    batch = next(iter(subset_loader))  # Load the batch\n",
    "    \n",
    "    # Perform prediction\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(T):\n",
    "            batch.to(device)\n",
    "            model.train(training)  # Enable/disable dropout\n",
    "            output = torch.softmax(model(batch), dim=-1)\n",
    "            outputs.append(output.cpu().numpy())\n",
    "    outputs = np.stack(outputs)\n",
    "    print(outputs.shape)\n",
    "    return outputs, random_subset\n",
    "\n",
    "def shannon_entropy_function(model, dataset, pool_indices, T=100, E_H=False, training=True):\n",
    "    \"\"\"\n",
    "    Compute the Shannon entropy and optionally E_H if needed for BALD.\n",
    "    \"\"\"\n",
    "    outputs, random_subset = predictions_from_pool(model, dataset, pool_indices, T, training)\n",
    "    pc = outputs.mean(axis=0)\n",
    "    H = (-pc * np.log(pc + 1e-10)).sum(axis=-1)  # Prevent log(0)\n",
    "\n",
    "    if E_H:\n",
    "        E = -np.mean(np.sum(outputs * np.log(outputs + 1e-10), axis=-1), axis=0)\n",
    "        return H, E, random_subset\n",
    "    return H, random_subset\n",
    "\n",
    "def max_entropy(model, dataset, pool_indices, n_query=10, T=100, training=True):\n",
    "    \"\"\"\n",
    "    Choose pool points that maximize the predictive entropy.\n",
    "    \"\"\"\n",
    "    acquisition, random_subset = shannon_entropy_function(model, dataset, pool_indices, T, training=training)\n",
    "    idx = (-acquisition).argsort()[:n_query]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "bec59d22cfd25cbd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training with Max Entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "644a17254074a03e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset, train_indices, val_indices, test_indices, pool_indices, num_classes, num_node_features = load_tu_data()\n",
    "model = GNN(num_node_features, num_classes).to(device)\n",
    "active_learning_loop(model, dataset, train_indices, pool_indices, val_indices, test_indices, max_entropy, epochs=200)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ec61af1d1265047f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b68ee0ff709dad61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
